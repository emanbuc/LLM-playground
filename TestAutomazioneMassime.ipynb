{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jAK_1dj2DwVJz648zF203iMA5iCWcd8R",
      "authorship_tag": "ABX9TyOO/0QNQ4zk+KzTd2wAG7e8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emanbuc/LLM-playground/blob/master/TestAutomazioneMassime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oe4cMkwrhB6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Estrazione Massima"
      ],
      "metadata": {
        "id": "MpTfOf9qsk7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDLZ90kjg-E9",
        "outputId": "52723df4-d2d7-44c7-c230-4a9870aa064a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting OpenAI\n",
            "  Downloading openai-1.30.5-py3-none-any.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from OpenAI) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from OpenAI)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from OpenAI) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from OpenAI) (4.11.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->OpenAI) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->OpenAI) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->OpenAI) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->OpenAI)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->OpenAI)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->OpenAI) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->OpenAI) (2.18.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Installing collected packages: h11, httpcore, bs4, httpx, OpenAI\n",
            "Successfully installed OpenAI-1.30.5 bs4-0.0.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0\n"
          ]
        }
      ],
      "source": [
        "!pip install OpenAI bs4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "oM7IQQcy8SvE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per caricare API KEY da variabili di ambiente\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DkTBku_imAaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#carica API KEY da secret di Google Colab\n",
        "from google.colab import userdata\n",
        "openai_key= userdata.get('openai-key-nomos-ai')"
      ],
      "metadata": {
        "id": "CAkkcOBRlQRi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai_key)"
      ],
      "metadata": {
        "id": "YrkAG7X-s7AK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2F87R-46P2h",
        "outputId": "9c55ce26-b9ea-402a-bd6d-ee5119fd7941"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_and_token_count(messages,\n",
        "                                   model=\"gpt-3.5-turbo\", #model=\"gpt-4-turbo-preview\", #model=\"gpt-3.5-turbo\",\n",
        "                                   temperature=0,\n",
        "                                   max_tokens=500):\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    token_dict = {\n",
        "'prompt_tokens':response.usage.prompt_tokens,\n",
        "'completion_tokens':response.usage.completion_tokens,\n",
        "'total_tokens':response.usage.total_tokens,\n",
        "    }\n",
        "\n",
        "    return content, token_dict"
      ],
      "metadata": {
        "id": "FrODL3_HqD6W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config= {\n",
        "    \"massime_path\": \"/gdrive/MyDrive/NomosAI/Massime/test09\",#\"/gdrive/MyDrive/NomosAI/Massime\",\n",
        "    \"sentenze_path\": \"gdrive/MyDrive/NomosAI/Docs-Condivisi-NomosAI/Test Gold Standard/\", #\"/gdrive/MyDrive/NomosAI/Sentenze\",\n",
        "    \"prompt_path\":\"/gdrive/MyDrive/NomosAI/Prompt\",\n",
        "    \"models\":[\"gpt-4o-2024-05-13\",\"gpt-4-turbo-2024-04-09\",\"gpt-3.5-turbo\"],\n",
        "    \"prompts\":[\"system_message_E.txt\"],#[\"system_message_A.txt\",\"system_message_B.txt\",\"system_message_C.txt\"],\n",
        "    \"debug\": False\n",
        "}"
      ],
      "metadata": {
        "id": "BIlTfJAV8sMs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config[\"massime_path\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qBiVd8Rc-q9M",
        "outputId": "f83aab0f-69c8-4b20-a034-0e917e775a09"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive/MyDrive/NomosAI/Massime/test08'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_file_dict_from_folder(folder_path):\n",
        "    import os\n",
        "    import glob\n",
        "    # Initialize an empty dictionary\n",
        "    text_files_dict = {}\n",
        "\n",
        "    # Create the pattern for .txt files\n",
        "    pattern = os.path.join(folder_path, '*.txt')\n",
        "\n",
        "    # Find all .txt files matching the pattern\n",
        "    for file_path in glob.glob(pattern):\n",
        "        # Extract the file name from the file path\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        # Read the content of the file\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Add the file name and content to the dictionary\n",
        "        text_files_dict[file_name] = content\n",
        "\n",
        "    return text_files_dict"
      ],
      "metadata": {
        "id": "TphVlFOE_qd1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_files_dict = create_text_file_dict_from_folder(config[\"prompt_path\"])\n",
        "for file_name, content in prompts_files_dict.items():\n",
        "    print(f'File: {file_name}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXPfN_Wq_zmE",
        "outputId": "082ab300-7ce7-413b-f680-8b88bd4f5c4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: system_message_A.txt\n",
            "\n",
            "File: system_message_B.txt\n",
            "\n",
            "File: system_message_C.txt\n",
            "\n",
            "File: system_message_D.txt\n",
            "\n",
            "File: system_message_E.txt\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_output_as_text_file(filename,text_content):\n",
        "  percorso_file = os.path.join(config[\"massime_path\"], filename)\n",
        "  with open(percorso_file+'.txt', 'w') as f:\n",
        "    f.write(text_content)"
      ],
      "metadata": {
        "id": "IwPuCwaZuCzF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genera_massima(prompt_file,sentenza,model,output_file_name):\n",
        "\n",
        "    prompt = prompts_files_dict[prompt_file]\n",
        "    messages = [\n",
        "      {'role':'system',\n",
        "      'content':prompt},\n",
        "      {'role':'user',\n",
        "      'content':sentenza},\n",
        "      ]\n",
        "\n",
        "    if(config[\"debug\"]):\n",
        "      print(\"====== promt: ========== \\n\")\n",
        "      print(prompt)\n",
        "\n",
        "      print(\" ======== sentenza =======\\n\")\n",
        "      print(sentenza)\n",
        "\n",
        "\n",
        "    response, token_dict = get_completion_and_token_count(messages,model=model,max_tokens= 1000)\n",
        "\n",
        "    if(config[\"debug\"]):\n",
        "      print(\" ======= response =========\\n\")\n",
        "\n",
        "      print(response)\n",
        "\n",
        "      print(\"======== tokens_dict ========= \\n\")\n",
        "\n",
        "      print(token_dict)\n",
        "    save_output_as_text_file(output_file_name+\"_\"+model+\"_\"+prompt_file,response)"
      ],
      "metadata": {
        "id": "fP5UlzsVDOkl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import os\n",
        "for radice, cartelle, file in os.walk(config[\"sentenze_path\"]):\n",
        "    for f in file:\n",
        "      if f.endswith('.html'):\n",
        "        percorso_file = os.path.join(radice, f)\n",
        "        with open(percorso_file, 'r', encoding='utf-8') as fh:\n",
        "          sentenza = fh.read()\n",
        "\n",
        "          print(\"Elaboro file: \"+ percorso_file + \"\\n\")\n",
        "\n",
        "          soup = BeautifulSoup(sentenza, 'html.parser')\n",
        "\n",
        "          # Estrai il testo da tutti i tag <p>\n",
        "          testoSentenza = ''\n",
        "          for paragrafo in soup.find_all('p'):\n",
        "              testoSentenza += paragrafo.text + '\\n'\n",
        "\n",
        "          save_output_as_text_file(f+\"_TESTO_SENTENZA\",testoSentenza)\n",
        "\n",
        "          for prompt_file in config[\"prompts\"]:\n",
        "            for model in config[\"models\"]:\n",
        "              try:\n",
        "                genera_massima(prompt_file,testoSentenza,model,f)\n",
        "              except:\n",
        "                print(\"ERRORE sentenza: \"+ percorso_file + \" MODEL: \"+model+\"+ PROMPT: \"+ prompt_file +\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7rWCK8I2pvA",
        "outputId": "b2b85f91-8016-4307-c25b-5d92a578dfd9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elaboro file: /gdrive/MyDrive/NomosAI/Docs-Condivisi-NomosAI/Test Gold Standard/18/18_202400011_11.html\n",
            "\n",
            "ERRORE sentenza: /gdrive/MyDrive/NomosAI/Docs-Condivisi-NomosAI/Test Gold Standard/18/18_202400011_11.html MODEL: gpt-3.5-turbo+ PROMPT: system_message_E.txt\n",
            "\n"
          ]
        }
      ]
    }
  ]
}