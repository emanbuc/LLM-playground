{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1jAK_1dj2DwVJz648zF203iMA5iCWcd8R",
      "authorship_tag": "ABX9TyNN3NvQi0ebf+uduR23+7Nh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emanbuc/LLM-playground/blob/master/TestAutomazioneMassime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oe4cMkwrhB6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Estrazione Massima"
      ],
      "metadata": {
        "id": "MpTfOf9qsk7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDLZ90kjg-E9"
      },
      "outputs": [],
      "source": [
        "!pip install OpenAI bs4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "oM7IQQcy8SvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per caricare API KEY da variabili di ambiente\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "_ = load_dotenv(find_dotenv()) # read local .env file\n",
        "\n",
        "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DkTBku_imAaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#carica API KEY da secret di Google Colab\n",
        "from google.colab import userdata\n",
        "openai_key= userdata.get('openai-key-nomos-ai')"
      ],
      "metadata": {
        "id": "CAkkcOBRlQRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=openai_key)"
      ],
      "metadata": {
        "id": "YrkAG7X-s7AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "id": "N2F87R-46P2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion_and_token_count(messages,\n",
        "                                   model=\"gpt-3.5-turbo\", #model=\"gpt-4-turbo-preview\", #model=\"gpt-3.5-turbo\",\n",
        "                                   temperature=0,\n",
        "                                   max_tokens=500):\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    token_dict = {\n",
        "'prompt_tokens':response.usage.prompt_tokens,\n",
        "'completion_tokens':response.usage.completion_tokens,\n",
        "'total_tokens':response.usage.total_tokens,\n",
        "    }\n",
        "\n",
        "    return content, token_dict"
      ],
      "metadata": {
        "id": "FrODL3_HqD6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config= {\n",
        "    \"massime_path\": \"/gdrive/MyDrive/NomosAI/Massime/test09\",#\"/gdrive/MyDrive/NomosAI/Massime\",\n",
        "    \"sentenze_path\": \"/gdrive/MyDrive/NomosAI/Docs-Condivisi-NomosAI/Test Gold Standard\", #\"/gdrive/MyDrive/NomosAI/Sentenze\",\n",
        "    \"prompt_path\":\"/gdrive/MyDrive/NomosAI/Prompt\",\n",
        "    \"models\":[\"gpt-4o-2024-05-13\",\"gpt-4-turbo-2024-04-09\",\"gpt-3.5-turbo\"],\n",
        "    \"prompts\":[\"system_message_E.txt\"],#[\"system_message_A.txt\",\"system_message_B.txt\",\"system_message_C.txt\"],\n",
        "    \"debug\": False\n",
        "}"
      ],
      "metadata": {
        "id": "BIlTfJAV8sMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config[\"massime_path\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qBiVd8Rc-q9M",
        "outputId": "f8b74b25-af3d-4a41-86df-8e06018b78c9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive/MyDrive/NomosAI/Massime/test09'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_text_file_dict_from_folder(folder_path):\n",
        "    import os\n",
        "    import glob\n",
        "    # Initialize an empty dictionary\n",
        "    text_files_dict = {}\n",
        "\n",
        "    # Create the pattern for .txt files\n",
        "    pattern = os.path.join(folder_path, '*.txt')\n",
        "\n",
        "    # Find all .txt files matching the pattern\n",
        "    for file_path in glob.glob(pattern):\n",
        "        # Extract the file name from the file path\n",
        "        file_name = os.path.basename(file_path)\n",
        "\n",
        "        # Read the content of the file\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "\n",
        "        # Add the file name and content to the dictionary\n",
        "        text_files_dict[file_name] = content\n",
        "\n",
        "    return text_files_dict"
      ],
      "metadata": {
        "id": "TphVlFOE_qd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_files_dict = create_text_file_dict_from_folder(config[\"prompt_path\"])\n",
        "for file_name, content in prompts_files_dict.items():\n",
        "    print(f'File: {file_name}\\n')"
      ],
      "metadata": {
        "id": "FXPfN_Wq_zmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_output_as_text_file(filename,text_content):\n",
        "  percorso_file = os.path.join(config[\"massime_path\"], filename)\n",
        "  with open(percorso_file+'.txt', 'w') as f:\n",
        "    f.write(text_content)"
      ],
      "metadata": {
        "id": "IwPuCwaZuCzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genera_massima(prompt_file,sentenza,model,output_file_name):\n",
        "\n",
        "    prompt = prompts_files_dict[prompt_file]\n",
        "    messages = [\n",
        "      {'role':'system',\n",
        "      'content':prompt},\n",
        "      {'role':'user',\n",
        "      'content':sentenza},\n",
        "      ]\n",
        "\n",
        "    if(config[\"debug\"]):\n",
        "      print(\"====== promt: ========== \\n\")\n",
        "      print(prompt)\n",
        "\n",
        "      print(\" ======== sentenza =======\\n\")\n",
        "      print(sentenza)\n",
        "\n",
        "\n",
        "    response, token_dict = get_completion_and_token_count(messages,model=model,max_tokens= 1000)\n",
        "\n",
        "    if(config[\"debug\"]):\n",
        "      print(\" ======= response =========\\n\")\n",
        "\n",
        "      print(response)\n",
        "\n",
        "      print(\"======== tokens_dict ========= \\n\")\n",
        "\n",
        "      print(token_dict)\n",
        "    save_output_as_text_file(output_file_name+\"_\"+model+\"_\"+prompt_file,response)"
      ],
      "metadata": {
        "id": "fP5UlzsVDOkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import os\n",
        "for radice, cartelle, file in os.walk(config[\"sentenze_path\"]):\n",
        "    for f in file:\n",
        "      if f.endswith('.html'):\n",
        "        percorso_file = os.path.join(radice, f)\n",
        "        with open(percorso_file, 'r', encoding='utf-8') as fh:\n",
        "          sentenza = fh.read()\n",
        "\n",
        "          print(\"Elaboro file: \"+ percorso_file + \"\\n\")\n",
        "\n",
        "          soup = BeautifulSoup(sentenza, 'html.parser')\n",
        "\n",
        "          # Estrai il testo da tutti i tag <p>\n",
        "          testoSentenza = ''\n",
        "          for paragrafo in soup.find_all('p'):\n",
        "              testoSentenza += paragrafo.text + '\\n'\n",
        "\n",
        "          save_output_as_text_file(f+\"_TESTO_SENTENZA\",testoSentenza)\n",
        "\n",
        "          for prompt_file in config[\"prompts\"]:\n",
        "            for model in config[\"models\"]:\n",
        "              try:\n",
        "                genera_massima(prompt_file,testoSentenza,model,f)\n",
        "              except:\n",
        "                print(\"ERRORE sentenza: \"+ percorso_file + \" MODEL: \"+model+\"+ PROMPT: \"+ prompt_file +\"\\n\")\n"
      ],
      "metadata": {
        "id": "G7rWCK8I2pvA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}